{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fff92a-a811-42b7-8dd1-69047f5bff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from opencv-python) (2.0.2)\n",
      "Preprocessed images saved in ..\\data\\SD1\\processed_train\n",
      "Preprocessed images saved in ..\\data\\SD1\\processed_val\n"
     ]
    }
   ],
   "source": [
    "# 01_data_preprocessing.ipynb\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install opencv-python\n",
    "\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "# Directory paths for SD1 dataset\n",
    "DATA_DIR = Path(\"../data/SD1\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR = DATA_DIR / \"val\"\n",
    "\n",
    "PROCESSED_TRAIN_DIR = DATA_DIR / \"processed_train\"\n",
    "PROCESSED_VAL_DIR = DATA_DIR / \"processed_val\"\n",
    "\n",
    "# Ensure processed directories exist\n",
    "PROCESSED_TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_VAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Image dimensions to split and resize\n",
    "GT_IMG_SIZE = (512, 512)  # Ground Truth Image size\n",
    "GLARE_IMG_SIZE = (512, 512)  # Glare Image size\n",
    "IMG_SIZE = (512, 512)  # Final output size for training\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    image_paths = glob.glob(str(input_dir / \"*.png\"))  # Assuming images are PNG format\n",
    "    for img_path in image_paths:\n",
    "        # Read image with RGBA channels (4 channels)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # Split the image into three parts: Ground Truth, Glare Image, and Glare Mask\n",
    "        ground_truth_img = img[:, :512]  # First part (512x512)\n",
    "        glare_img = img[:, 512:1024]  # Second part (512x512)\n",
    "        \n",
    "        # Convert both images to grayscale (1 channel)\n",
    "        ground_truth_img_gray = cv2.cvtColor(ground_truth_img, cv2.COLOR_RGBA2GRAY)\n",
    "        glare_img_gray = cv2.cvtColor(glare_img, cv2.COLOR_RGBA2GRAY)\n",
    "        \n",
    "        # Resize to match the final output size (if required)\n",
    "        ground_truth_img_gray = cv2.resize(ground_truth_img_gray, GT_IMG_SIZE)\n",
    "        glare_img_gray = cv2.resize(glare_img_gray, GLARE_IMG_SIZE)\n",
    "        \n",
    "        # Save the processed Ground Truth and Glare Images (grayscale)\n",
    "        ground_truth_save_path = output_dir / f\"gt_{Path(img_path).name}\"\n",
    "        glare_save_path = output_dir / f\"glare_{Path(img_path).name}\"\n",
    "        \n",
    "        cv2.imwrite(str(ground_truth_save_path), ground_truth_img_gray)\n",
    "        cv2.imwrite(str(glare_save_path), glare_img_gray)\n",
    "    \n",
    "    print(f\"Preprocessed images saved in {output_dir}\")\n",
    "\n",
    "# Preprocess training and validation images\n",
    "preprocess_images(TRAIN_DIR, PROCESSED_TRAIN_DIR)\n",
    "preprocess_images(VAL_DIR, PROCESSED_VAL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b23f19-d735-4a8f-9ad6-a0158255f0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
