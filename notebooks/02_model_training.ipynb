{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135ea1d-7ac0-402d-b056-3248d94833f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|                                                                                                                                                                                      | 0/750 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install tqdm torch torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Directory paths for SD1 dataset\n",
    "DATA_DIR = Path(\"../data/SD1\")\n",
    "PROCESSED_TRAIN_DIR = DATA_DIR / \"processed_train\"\n",
    "PROCESSED_VAL_DIR = DATA_DIR / \"processed_val\"\n",
    "\n",
    "# Custom dataset class for paired images\n",
    "class GlareRemovalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, limit=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.image_pairs = []\n",
    "\n",
    "        # Get all `glare_xxx.png` and match them with `gt_xxx.png`\n",
    "        glare_images = sorted(self.root_dir.glob(\"glare_*.png\"))\n",
    "        gt_images = sorted(self.root_dir.glob(\"gt_*.png\"))\n",
    "\n",
    "        # Ensure paired dataset\n",
    "        for glare_img in glare_images:\n",
    "            gt_img = self.root_dir / glare_img.name.replace(\"glare_\", \"gt_\")\n",
    "            if gt_img.exists():\n",
    "                self.image_pairs.append((glare_img, gt_img))\n",
    "\n",
    "        # Limit dataset size for debugging\n",
    "        if limit:\n",
    "            self.image_pairs = random.sample(self.image_pairs, min(limit, len(self.image_pairs)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        glare_path, gt_path = self.image_pairs[idx]\n",
    "        glare_img = Image.open(glare_path).convert(\"RGB\")\n",
    "        gt_img = Image.open(gt_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            glare_img = self.transform(glare_img)\n",
    "            gt_img = self.transform(gt_img)\n",
    "\n",
    "        return glare_img, gt_img\n",
    "\n",
    "# Transformations (No need for resizing since images are already 512x512)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "train_dataset = GlareRemovalDataset(PROCESSED_TRAIN_DIR, transform=transform)\n",
    "val_dataset = GlareRemovalDataset(PROCESSED_VAL_DIR, transform=transform)\n",
    "\n",
    "# Increase Batch Size (If Memory Allows) to process more images at once\n",
    "# Optimize DataLoader (num_workers > 0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define a smaller model (DeepLabV3-MobileNetV3)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "\n",
    "        # Change the last layer to output 3 channels instead of 21\n",
    "        self.model.classifier[4] = nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "\n",
    "# Instantiate the model and move to device\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.L1Loss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Ensure outputs match target size (output is already 512x512)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint_path = f\"../models/unet__mobilenet_epoch_{epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = \"../models/final_unet_mobilenet.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370a1fa-7174-4cca-a02a-fc59f6bdc165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
