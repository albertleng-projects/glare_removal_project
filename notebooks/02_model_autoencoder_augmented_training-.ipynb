{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1df8f-e4b1-4a6f-b40a-00ba8f9627f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: pytorch-msssim in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\albert.leng\\pycharmprojects\\glare_removal_project\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [15:02<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3755\n",
      "Epoch 1 - Validation Loss: 0.2316\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [15:09<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2482\n",
      "Epoch 2 - Validation Loss: 0.2239\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [15:05<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2427\n",
      "Epoch 3 - Validation Loss: 0.2243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [15:12<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 0.2418\n",
      "Epoch 4 - Validation Loss: 0.2154\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [15:06<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 0.2418\n",
      "Epoch 5 - Validation Loss: 0.2153\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [15:33<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training Loss: 0.2415\n",
      "Epoch 6 - Validation Loss: 0.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [16:37<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training Loss: 0.2411\n",
      "Epoch 7 - Validation Loss: 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [18:19<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training Loss: 0.2414\n",
      "Epoch 8 - Validation Loss: 0.2108\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [20:14<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 0.2407\n",
      "Epoch 9 - Validation Loss: 0.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [20:18<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training Loss: 0.2401\n",
      "Epoch 10 - Validation Loss: 0.2204\n",
      "📌 Checkpoint saved at: ..\\models\\glare_autoencoder_augmented_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [20:45<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training Loss: 0.2403\n",
      "Epoch 11 - Validation Loss: 0.2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [20:42<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training Loss: 0.2397\n",
      "Epoch 12 - Validation Loss: 0.2080\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [21:24<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training Loss: 0.2395\n",
      "Epoch 13 - Validation Loss: 0.2078\n",
      "✅ Best Model Saved at: ..\\models\\best_glare_removal_autoencoder_augmented.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [22:48<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training Loss: 0.2392\n",
      "Epoch 14 - Validation Loss: 0.2197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [24:20<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training Loss: 0.2394\n",
      "Epoch 15 - Validation Loss: 0.2158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [24:36<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training Loss: 0.2387\n",
      "Epoch 16 - Validation Loss: 0.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [25:24<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training Loss: 0.2388\n",
      "Epoch 17 - Validation Loss: 0.2113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [28:44<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training Loss: 0.2389\n",
      "Epoch 18 - Validation Loss: 0.2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [27:15<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training Loss: 0.2394\n",
      "Epoch 19 - Validation Loss: 0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [26:48<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training Loss: 0.2386\n",
      "Epoch 20 - Validation Loss: 0.2125\n",
      "📌 Checkpoint saved at: ..\\models\\glare_autoencoder_augmented_epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [25:27<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training Loss: 0.2389\n",
      "Epoch 21 - Validation Loss: 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [25:45<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training Loss: 0.2388\n",
      "Epoch 22 - Validation Loss: 0.2133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [29:43<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training Loss: 0.2386\n",
      "Epoch 23 - Validation Loss: 0.2122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30:   1%|█▌                                                                                                                                                                          | 7/750 [00:16<29:28,  2.38s/it]"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install tqdm torch torchvision pytorch-msssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_msssim import ssim\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from models.autoencoder.auto import Auto  # Import your custom Auto class\n",
    "\n",
    "# ✅ Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Paths\n",
    "DATA_DIR = Path(\"../data/SD1\")\n",
    "PROCESSED_TRAIN_DIR = DATA_DIR / \"processed_train\"\n",
    "PROCESSED_VAL_DIR = DATA_DIR / \"processed_val\"\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)  # Ensure model directory exists\n",
    "\n",
    "# ✅ Dataset class\n",
    "class GlareRemovalDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform: transforms.Compose = None, limit: int = None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.image_pairs = []\n",
    "\n",
    "        glare_images = sorted(self.root_dir.glob(\"glare_*.png\"))\n",
    "        gt_images = sorted(self.root_dir.glob(\"gt_*.png\"))\n",
    "\n",
    "        for glare_img in glare_images:\n",
    "            gt_img = self.root_dir / glare_img.name.replace(\"glare_\", \"gt_\")\n",
    "            if gt_img.exists():\n",
    "                self.image_pairs.append((glare_img, gt_img))\n",
    "\n",
    "        if limit:\n",
    "            self.image_pairs = random.sample(self.image_pairs, min(limit, len(self.image_pairs)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        glare_path, gt_path = self.image_pairs[idx]\n",
    "        glare_img = Image.open(glare_path).convert(\"RGB\")\n",
    "        gt_img = Image.open(gt_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            glare_img = self.transform(glare_img)\n",
    "            gt_img = self.transform(gt_img)\n",
    "\n",
    "        return glare_img, gt_img\n",
    "\n",
    "# ✅ Transformations with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ✅ Data loaders (Optimized)\n",
    "train_dataset = GlareRemovalDataset(PROCESSED_TRAIN_DIR, transform=transform)\n",
    "val_dataset = GlareRemovalDataset(PROCESSED_VAL_DIR, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# ✅ Instantiate Model\n",
    "model = Auto().to(device)\n",
    "\n",
    "# ✅ Custom Loss (L1 + SSIM Loss)\n",
    "def custom_loss(predicted: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    l1_loss = F.l1_loss(predicted, target)\n",
    "    ssim_loss = 1 - ssim(predicted, target, data_range=1, size_average=True)\n",
    "    return 0.5 * l1_loss + 0.5 * ssim_loss\n",
    "\n",
    "# ✅ Optimizer & Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "\n",
    "# ✅ Training loop\n",
    "num_epochs = 30  # Increased for better learning\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 15\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = custom_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # ✅ Gradient Clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # ✅ Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += custom_loss(outputs, targets).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ✅ Adjust learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # ✅ Early stopping and model saving\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        best_model_path = MODEL_DIR / \"best_glare_removal_autoencoder_augmented.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Best Model Saved at: {best_model_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    # ✅ Periodic checkpoint saving\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = MODEL_DIR / f\"glare_autoencoder_augmented_epoch_{epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"📌 Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "# ✅ Save final model\n",
    "final_model_path = MODEL_DIR / \"final_glare_removal_autoencoder_augemented.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"🎉 Final Model Saved at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b415fd4-a1b0-47cc-b761-57b7b6bb0c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
