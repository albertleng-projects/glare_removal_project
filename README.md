# Glare Removal Project

This repository contains an implementation of a **glare removal model** using an **autoencoder**. The project is
structured into two main components:

1. **`api/`** ‚Äì A Flask-based API for model inference.
2. **`notebooks/`** ‚Äì Jupyter notebooks for data preprocessing, model training, evaluation, and inference.

The model is trained using **an autoencoder** located in `models/autoencoder/`. The **goal** is to process images
affected by glare and generate an enhanced, glare-free version.

---

## üìÇ Repository Structure

```graphql
glare_removal_project/
‚îÇ‚îÄ‚îÄ api/                # Flask API for model inference
‚îÇ‚îÄ‚îÄ data/               # Data used for training (ignored in Git)
‚îÇ‚îÄ‚îÄ models/             # Trained models & autoencoder architecture
‚îÇ‚îÄ‚îÄ notebooks/          # Jupyter notebooks for training and evaluation
‚îÇ‚îÄ‚îÄ Dockerfile          # Containerization setup for the API
‚îÇ‚îÄ‚îÄ README.md           # Documentation
‚îÇ‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îî‚îÄ‚îÄ .gitignore          # Files to be ignored in version control
```

---

## üöÄ How to Use This Repository

### 1Ô∏è‚É£ Clone the Repository

First, clone this repository to your local machine:

```bash
git clone https://github.com/albertleng-projects/glare_removal_project.git
cd glare_removal_project
```

### 2Ô∏è‚É£ Set Up a Virtual Environment

It‚Äôs recommended to use a virtual environment to manage dependencies.

For Windows (PowerShell):

```bash
python -m venv venv
venv\Scripts\activate
```

For Mac/Linux:

```bash 
python -m venv venv
source venv/bin/activate
```

### 3Ô∏è‚É£ Install Dependencies

Once inside the virtual environment, install the required packages:

```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Run Jupyter Notebooks (For Training & Evaluation)

If you want to explore the data preprocessing, model training, and evaluation, start Jupyter Notebook:

```bash
jupyter notebook
```

## üìù Notebooks Overview

### 1Ô∏è‚É£ **Data Preprocessing ([01_data_preprocessing.ipynb](notebooks/01_data_preprocessing.ipynb))**

- Loads the dataset from `data/` (not included in the repo due to large size).
- Applies transformations such as resizing, normalization, and augmentation.
- Splits data into **training** and **validation** sets.

### 2Ô∏è‚É£ **Model Training ([02_model_training.ipynb](notebooks/02_model_training.ipynb))**

- Defines the **autoencoder architecture** ([models/autoencoder/auto.py](models/autoencoder/auto.py)).
- Trains the model using **L1 loss** (Mean Absolute Error).
- Uses **Adam optimizer** for weight updates.
- Monitors **training loss** and **validation loss** per epoch.

### 3Ô∏è‚É£ **Model Evaluation ([03_model_evaluation.ipynb](notebooks/03_model_evaluation.ipynb))**

- Loads the trained model.
- Computes evaluation metrics (**L1 loss** on test images).
- Displays **sample predictions** (before and after glare removal).
- Identifies potential areas for improvement.

### 4Ô∏è‚É£ **Model Inference ([04_inference.ipynb](notebooks/04_inference.ipynb))**

- Loads a saved model ([models/final_glare_removal_autoencoder.pth](models/final_glare_removal_autoencoder.pth)).
- Runs inference on new input images.
- Saves and displays the enhanced (glare-free) images.

---

## üèóÔ∏è Model Details

- The project uses a **convolutional autoencoder** for glare removal.
- The encoder extracts features from the input image.
- The decoder reconstructs the glare-free image.
- The model is trained using an **L1 loss function**, as it is well-suited for image restoration tasks.

**Training Results:**

- The final **training loss** and **validation loss** per epoch are shown in **Results**
  of [03_model_evaluation.ipynb](notebooks/03_model_evaluation.ipynb).
- Lower **L1 loss** indicates better performance.
- The **validation loss** achieved is **0.1388**, which does not meet the target of **0.06** due to constraints in time
  and resources. That would be addressed in future enhancements.

---

## üöÄ Using the API

The `api/` folder contains a Flask-based API to serve the trained model.

See the [api/README.md](./api/README.md) for detailed instructions on setting up and running the API.

## üîÆ Future Enhancements

### 1. Improve Model Performance

- Experiment with more **complex architectures** (e.g., UNet, GANs).
- Tune **hyperparameters** (learning rate, batch size, etc.).

### 2. Better Training Dataset

- Collect more **diverse glare-affected images**.
- Augment dataset with **synthetic glare effects**.

### 3. Optimize API Performance and Use GPU

- Use **FastAPI** instead of Flask for faster inference.
- Deploy model using **TensorRT** for optimized GPU execution.
- Utilize **GPU** (e.g., Google Colab) for faster model training and inference.

### 4. Deploy to Cloud

- Host API on **AWS/GCP/Azure** with **Docker & Kubernetes**.
- Use **CI/CD pipelines** for automated deployment.

